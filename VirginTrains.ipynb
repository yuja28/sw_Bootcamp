{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KI1JY-e6ihAI45QL8uQWBpdIcnyt14Zd",
      "authorship_tag": "ABX9TyOEhkFbDvOooLg9uv0mqCjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuja28/sw_Bootcamp/blob/main/VirginTrains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ltNAfwO00Tw",
        "outputId": "d8dc2ca1-7790-4e18-faed-1c1a1d92d596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VirginTrains Support: Welcome to VirginTrains Support. I will answer your queries about VirginTrains. If you wish to end the chat, type bye!\n",
            "Can I cancel my ticket?\n",
            "VirginTrains Support: i see, how did you cancel this?\n",
            "What is the ticket cancellation policy?\n",
            "VirginTrains Support: we have a 28 day policy for delay repay.\n",
            "bye\n",
            "VirginTrains Support: Thanks for chatting. I hope we could assist you today.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/customer-support-on-twitter/input/twcs/twcs.csv\")\n",
        "df.head()\n",
        "\n",
        "#Specify which company we want to work with\n",
        "company = \"VirginTrains\"\n",
        "\n",
        "#Filter for answers only made by that company\n",
        "answers = df.loc[df['author_id'] == company]\n",
        "\n",
        "df_text=df[['text']]\n",
        "df_text.head()\n",
        "\n",
        "#Convert all our text to lower case\n",
        "answers['text'] = answers.apply(lambda row: row['text'].lower(), axis=1)\n",
        "#Strip off any trailing full stops\n",
        "answers['text'] = answers.apply(lambda row: row['text'].rstrip('.'), axis=1)\n",
        "#Remove any mentions to users e.g. \"@johnsmith you can do this by....\"\n",
        "answers['text'] = answers.apply(lambda row: re.sub(\"\\B@\\w+\", \"\", row['text']), axis=1)\n",
        "\n",
        "#variable for concatinating all answers sent by the company\n",
        "raw = \"\"\n",
        "\n",
        "#concatinate answers into raw variable\n",
        "for index, row in answers.iterrows():\n",
        "    raw += \". \" + row['text']\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#convert our raw sentences into sentence tokens\n",
        "sentence_tokens = nltk.sent_tokenize(raw)\n",
        "#convert our raw sentences into word tokens\n",
        "word_tokens = nltk.word_tokenize(raw);\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "#import necessary libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#define our function for processing a response\n",
        "def response(user_response):\n",
        "    #define our response variable\n",
        "    robo_response=''\n",
        "    #add our users input as a response\n",
        "    sentence_tokens.append(user_response)\n",
        "    #create out vectorizer\n",
        "    vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    #process our tokens\n",
        "    diff = vectorizer.fit_transform(sentence_tokens)\n",
        "    #find the similarity\n",
        "    vals = cosine_similarity(diff[-1], diff)\n",
        "    #select our sentence\n",
        "    idx = vals.argsort()[0][-2]\n",
        "    #calculate accuracy\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_diff = flat[-2]\n",
        "    if(req_diff==0):\n",
        "        #if no appropriate response can be made\n",
        "        robo_response=robo_response+\"Sorry! I don't think I can help you with that.\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        #if an appropriate response is found\n",
        "        robo_response = sentence_tokens[idx]\n",
        "        return robo_response\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Tokenization using nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize and remove stop words\n",
        "texts = [[word for word in nltk.word_tokenize(text.lower()) if word.isalnum() and word not in stop_words] for text in sentence_tokens]\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# LDA model training\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=5, # Specify the number of topics (you can change this)\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "# Print the main topics\n",
        "print(\"{companyname} Support: Welcome to {companyname} Support. I will answer your queries about {companyname}. If you wish to end the chat, type bye!\".format(companyname = company))\n",
        "while True:\n",
        "    #get an input\n",
        "    user_response = input()\n",
        "    #convert to lower\n",
        "    user_response=user_response.lower()\n",
        "    #if they type something other than 'bye'\n",
        "    if user_response != 'bye':\n",
        "        #show bot is typing\n",
        "        print(\"{companyname} Support: \".format(companyname = company), end=\"\")\n",
        "        #print our AI response\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "    else:\n",
        "        #exit the loop\n",
        "        print(\"{companyname} Support: Thanks for chatting. I hope we could assist you today.\".format(companyname = company))\n",
        "        break\n"
      ]
    }
  ]
}